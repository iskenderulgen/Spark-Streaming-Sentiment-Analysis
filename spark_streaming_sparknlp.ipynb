{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d6908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace, col\n",
    "from sparknlp.base import DocumentAssembler, Pipeline, EmbeddingsFinisher\n",
    "from sparknlp.annotator import Tokenizer, WordEmbeddingsModel, SentenceEmbeddings\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Note: Spark-nlp downloads huge amount of jar files, this might take a while\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"Spark-Text-Classification\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"8G\")\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.5.3\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afd1a2a",
   "metadata": {},
   "source": [
    "# Read IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758ac644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/stanfordnlp/imdb\n",
    "\n",
    "imdb_dataset = spark.read.parquet(\"data/train-imdb.parquet\").withColumn(\n",
    "    \"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \"\")\n",
    ")\n",
    "\n",
    "# Create a window partitioned by label and ordered randomly\n",
    "windowSpec = Window.partitionBy(\"label\").orderBy(F.rand())\n",
    "\n",
    "\"\"\"\n",
    "# Add a row number per label and filter to keep only the first 1000 rows per class\n",
    "# This ensures for efficiency and balance in the dataset for testing. \n",
    "# For production, you might want to use full dataset\n",
    "\"\"\"\n",
    "imdb_dataset = (\n",
    "    imdb_dataset.withColumn(\"row_num\", F.row_number().over(windowSpec))\n",
    "    .filter(F.col(\"row_num\") <= 3000)\n",
    "    .drop(\"row_num\")\n",
    ")\n",
    "\n",
    "# Show the sampled dataset\n",
    "imdb_dataset.show(5, truncate=50)\n",
    "\n",
    "print(\n",
    "    \"Number of classes in the sampled dataset: \",\n",
    "    imdb_dataset.select(\"label\").distinct().count(),\n",
    "    \"total number of rows: \",\n",
    "    imdb_dataset.count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d71ac",
   "metadata": {},
   "source": [
    "# Pipeline for Tokenizing and Embedding Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb23afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Spark NLP pipeline stages\n",
    "\n",
    "# 1. DocumentAssembler converts raw text into a document annotation.\n",
    "document_assembler = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
    "\n",
    "# 2. Tokenizer splits the document into tokens.\n",
    "tokenizer = Tokenizer().setInputCols([\"document\"]).setOutputCol(\"token\")\n",
    "\n",
    "# 3. Load pre-trained GloVe embeddings\n",
    "# 3. Load GloVe embeddings from local file\n",
    "word_embeddings = (\n",
    "    WordEmbeddingsModel.load(\"data/glove_100d\")\n",
    "    .setInputCols([\"document\", \"token\"])\n",
    "    .setOutputCol(\"embeddings\")\n",
    ")\n",
    "\n",
    "# 4. Create sentence-level embeddings by averaging the word embeddings.\n",
    "sentence_embeddings = (\n",
    "    SentenceEmbeddings()\n",
    "    .setInputCols([\"document\", \"embeddings\"])\n",
    "    .setOutputCol(\"sentence_embeddings\")\n",
    "    .setPoolingStrategy(\"AVERAGE\")\n",
    ")\n",
    "\n",
    "# 5. Finisher converts NLP annotations into plain array column\n",
    "finisher = (\n",
    "    EmbeddingsFinisher()\n",
    "    .setInputCols(\"sentence_embeddings\")\n",
    "    .setOutputCols(\"features\")\n",
    "    .setOutputAsVector(True)\n",
    "    .setCleanAnnotations(True)\n",
    ")\n",
    "\n",
    "# Build the pipeline\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[\n",
    "        document_assembler,\n",
    "        tokenizer,\n",
    "        word_embeddings,\n",
    "        sentence_embeddings,\n",
    "        finisher,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e6a29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform the data\n",
    "\n",
    "fitted_pipeline = nlp_pipeline.fit(imdb_dataset)\n",
    "\n",
    "final_data = fitted_pipeline.transform(imdb_dataset).selectExpr(\n",
    "    \"label\", \"explode(features) as features\"\n",
    ")\n",
    "\n",
    "final_data.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca15e62",
   "metadata": {},
   "source": [
    "# Train logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d298a5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training (80%) and testing (20%) sets\n",
    "train_data, test_data = final_data.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Configure the LogisticRegression model from spark-rapids-ml\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50)\n",
    "\n",
    "# Fit the model using the training data\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate test accuracy; compare the predicted labels with numeric labels (\"label_index\")\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ed7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(5, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c7bdbe",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming for Text Classification\n",
    "## Plotting and Embedding Pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dab811",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_pos_counts, batch_neg_counts, batch_ids = [], [], []\n",
    "\n",
    "\n",
    "def plot_sentiment(pos_count, neg_count, batch_id):\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(batch_ids, batch_pos_counts, \"g-o\", label=\"Positive\")\n",
    "    plt.plot(batch_ids, batch_neg_counts, \"r-o\", label=\"Negative\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Sentiment Analysis Results by Batch\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    labels = [\"Positive\", \"Negative\"]\n",
    "    sizes = [pos_count, neg_count]\n",
    "    colors = [\"green\", \"red\"]\n",
    "    plt.pie(sizes, labels=labels, colors=colors, autopct=\"%1.1f%%\", startangle=90)\n",
    "    plt.axis(\"equal\")\n",
    "    plt.title(f\"Batch {batch_id} Sentiment Distribution\")\n",
    "    plt.tight_layout()\n",
    "    clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def process_batch(df, batch_id):\n",
    "    if df.count() == 0:\n",
    "        print(f\"Batch {batch_id}: Empty batch\")\n",
    "        return\n",
    "\n",
    "    df = df.withColumn(\"text\", regexp_replace(col(\"text\"), \"[^a-zA-Z0-9\\\\s]\", \"\"))\n",
    "\n",
    "    # Apply the Spark NLP pipeline to get sentence embeddings\n",
    "    embeddings_df = fitted_pipeline.transform(df).selectExpr(\n",
    "        \"explode(features) as features\"\n",
    "    )\n",
    "\n",
    "    # Predict using the trained Spark ML model\n",
    "    predictions = model.transform(embeddings_df)\n",
    "\n",
    "    # Collect predictions to driver for plotting\n",
    "    counts = predictions.groupBy(\"prediction\").count().toPandas()\n",
    "    pos_count = (\n",
    "        counts[counts[\"prediction\"] == 1][\"count\"].sum()\n",
    "        if 1 in counts[\"prediction\"].values\n",
    "        else 0\n",
    "    )\n",
    "    neg_count = (\n",
    "        counts[counts[\"prediction\"] == 0][\"count\"].sum()\n",
    "        if 0 in counts[\"prediction\"].values\n",
    "        else 0\n",
    "    )\n",
    "    batch_pos_counts.append(pos_count)\n",
    "    batch_neg_counts.append(neg_count)\n",
    "    batch_ids.append(batch_id)\n",
    "    plot_sentiment(pos_count, neg_count, batch_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d1ded4",
   "metadata": {},
   "source": [
    "## Spark Structured Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fb04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_stream = (\n",
    "    spark.readStream.format(\"parquet\")\n",
    "    .schema(StructType([StructField(\"text\", StringType(), True)]))\n",
    "    .option(\"maxFilesPerTrigger\", 1)\n",
    "    .load(\"data/yelp/\")\n",
    ")\n",
    "\n",
    "query = (\n",
    "    yelp_stream.writeStream\n",
    "    .foreachBatch(process_batch)\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    ")\n",
    "\n",
    "try:\n",
    "    query.awaitTermination(timeout=600)\n",
    "    print(\"Query terminated after timeout\")\n",
    "except Exception as e:\n",
    "    print(f\"Query terminated due to: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sparknlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
